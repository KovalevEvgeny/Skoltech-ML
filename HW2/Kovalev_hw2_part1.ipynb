{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 1 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **BEGIN SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* The are problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (1 pt.): Information criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that regression model is\n",
    "$$y = \\sum_{i=1}^k \\beta_i x_i + \\varepsilon,$$\n",
    "and $\\varepsilon$ is dictributed normally: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, $\\sigma^2$ is known.\n",
    "\n",
    "Prove that the model with highest Akaike information criterion is the model with smallest Mallow's $C_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "AIC optimization:\n",
    "\n",
    "$$\n",
    "AIC(S) = \\ell_J\\left(\\hat{\\beta}\\right) - |J| \\rightarrow \\max\\limits_{\\hat{\\beta}, J}\n",
    "$$\n",
    "\n",
    "This problem is equivalent to the following:\n",
    "\n",
    "$$\n",
    "-AIC(S) = -\\ell_J\\left(\\hat{\\beta}\\right) + |J| \\rightarrow \\min\\limits_{\\hat{\\beta}, J}\n",
    "$$\n",
    "\n",
    "Notice that:\n",
    "\n",
    "$$\n",
    "\\varepsilon_i \\mid X_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right) \\Rightarrow Y_i\\mid X_i \\sim \\mathcal{N}\\left(\\mu_i, \\sigma^2\\right),\\quad\\text{where } \\mu_i = \\sum\\limits_{j \\in J}\\hat{\\beta}_jX_{ij}\n",
    "$$\n",
    "\n",
    "Likelihood function:\n",
    "\n",
    "$$\n",
    "L_J\\left(X, Y \\mid \\hat{\\beta}\\right) = \\prod\\limits_{i = 1}^nf\\left(X_i, Y_i \\mid \\hat{\\beta}\\right) = \\prod\\limits_{i = 1}^nf_X\\left(X_i \\mid \\hat{\\beta}\\right)f_{Y\\mid X}\\left(Y_i\\mid X_i, \\hat{\\beta}\\right)\n",
    "$$\n",
    "\n",
    "Logarithmic likelihood function:\n",
    "\n",
    "$$\n",
    "\\ell_J\\left(X, Y \\mid \\hat{\\beta}\\right) = \\ln L_J\\left(X, Y \\mid \\hat{\\beta}\\right) = \\sum\\limits_{i = 1}^n\\ln f_X\\left(X_i \\mid \\hat{\\beta}\\right) + \\sum\\limits_{i = 1}^n\\ln f_{Y\\mid X}\\left(Y_i\\mid X_i, \\hat{\\beta}\\right)\n",
    "$$\n",
    "\n",
    "Notice that the first sum doesn't depend on $\\hat{\\beta}$, therefore we may cancel it out from the optimization problem. Let's take a look at the second one:\n",
    "\n",
    "$$\n",
    "\\ell_J\\left(\\hat{\\beta}\\right) = \\sum\\limits_{i = 1}^n\\ln f_{Y\\mid X}\\left(Y_i\\mid X_i, \\hat{\\beta}\\right) = \\sum\\limits_{i = 1}^n\\ln \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(Y_i - \\mu_i)^2}{2\\sigma^2}}\\right) = -n\\ln\\left(\\sqrt{2\\pi}\\sigma\\right) - \\frac{1}{2\\sigma^2}\\sum\\limits_{i = 1}^n\\left(Y_i - \\mu_i\\right)^2\n",
    "$$\n",
    "\n",
    "Here we can also cancel out the first summand.\n",
    "\n",
    "Let's get back to the task:\n",
    "\n",
    "$$\n",
    "-l_S\\left(\\hat{\\beta}\\right) + |J| \\rightarrow \\min\\limits_{\\hat{\\beta}, J} \\sim \\frac{1}{2\\sigma^2}\\sum\\limits_{i = 1}^n\\left(Y_i - \\mu_i\\right)^2 + |J| \\rightarrow \\min\\limits_{\\hat{\\beta}, J} \\sim\\frac{n}{2\\sigma^2}\\hat{R}_{tr}\\left(J\\right) + |J| \\rightarrow \\min\\limits_{\\hat{\\beta}, J} \\sim\\hat{R}_{tr}\\left(J\\right) + \\frac{2\\sigma^2}{n}|J| \\rightarrow \\min\\limits_{\\hat{\\beta}, J},\n",
    "$$\n",
    "\n",
    "what is indeed a Mallow $C_p$ optimization problem.\n",
    "\n",
    "Thus, AIC and Mallow $C_p$ optimization problems are equivalent, which means that the model with the highest AIC is the model with the smallest Mallow's $C_p$.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: gradient boosting, adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting and its theory\n",
    "\n",
    "Minimization of the loss function is an optimization task, and \"Gradient Boosting\"\n",
    "is one of the many methods to perform optimization. It shoould be noted that it\n",
    "uses *greedy* approach and therefore, like greedy algorithms in CS, may produce\n",
    "results that are not *globally* optimal.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & b_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\gamma_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & a_N(x) = \\sum_{n=0}^N \\gamma_n b_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm\n",
    "\n",
    "Consider a loss loss function $L(y, z)$ for target $y$ and prediction $z$, and let\n",
    "$(x_i, y_i)_{i=1}^l$ be our train dataset for a regression task. \n",
    "\n",
    "\n",
    "1. Initialize $a_0(x) = \\hat{z}$ with the *flat constant prediction*\n",
    "$\\hat{z} = \\arg\\min\\limits_{z \\in \\mathbb{R}} \\sum_{i=1}^l L(y_i, z)$;\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $G_n(b_n, \\gamma_n) \\to \\min\\limits_{b_{n}, \\gamma_n}$\n",
    "    where \n",
    "    $$ G_n(b, \\gamma) = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr) $$\n",
    "    with the following method:\n",
    "    \\begin{align}\n",
    "      & s_i = - \\frac{\\partial}{\\partial z} L(y_i, z) \\Big\\vert_{z=a_{n-1}(x_i)}\n",
    "          \\\\\n",
    "      & b_n(x) = \\arg\\min\\limits_{b\\in\\mathcal{A}}\\sum_{i=1}^l \\bigl(b(x_i) - s_i\\bigr)^2\n",
    "          \\\\\n",
    "      & \\gamma_n = \\arg\\min_\\gamma G_n(b_n, \\gamma)\n",
    "          \\\\\n",
    "      & a_n(x) = a_{n-1}(x) + \\gamma_n b_n(x)\n",
    "    \\end{align}\n",
    "3. return $a_N(x) = a_0(x) + \\sum_{n=1}^N \\gamma_n b_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (1 pt.)\n",
    "\n",
    "At the $n$-th step of Garient Boosting ($n \\geq 1$) we compute the \"residuals\"\n",
    "$(s_i)_{i=1}^l$ and learn $x\\mapsto b_n(x)$ with a regression algorithm $\\mathcal{A}$\n",
    "applied to the dataset $(x_i, s_i)_{i=1}^l$. For the next two tasks **assume\n",
    "that we have already perfomed these substeps**.\n",
    "\n",
    "Derive the **optimal value** of $\\gamma_n$ for *MSE* loss function $L(y, z) = \\tfrac12 (y - z)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "$$\n",
    "G_n\\left(b_n, \\gamma\\right) = \\sum\\limits_{i=1}^l L\\left(y_i, a_{n - 1}(x_i) + \\gamma b_n(x_i)\\right) = \\frac{1}{2}\\sum\\limits_{i=1}^l\\left(y_i - a_{n - 1}(x_i) - \\gamma b_n(x_i)\\right)^2\n",
    "$$\n",
    "$$\n",
    "\\gamma_n = \\arg\\min_\\gamma G_n\\left(b_n, \\gamma\\right)\n",
    "$$\n",
    "To find optimal $\\gamma_n$, let's make the derivative of $G_n\\left(b_n, \\gamma\\right)$ equal to zero:\n",
    "$$\n",
    "\\frac{\\partial G_n\\left(b_n, \\gamma\\right)}{\\partial \\gamma} = -\\sum\\limits_{i=1}^lb_n(x_i)\\left(y_i - a_{n-1}(x_i) - \\gamma b_n(x_i)\\right) = 0 \\Rightarrow \\gamma\\sum\\limits_{i=1}^lb_n^2(x_i) = \\sum\\limits_{i=1}^lb_n(x_i)\\left(y_i - a_{n-1}(x_i)\\right) \\Rightarrow \\gamma = \\frac{\\sum\\limits_{i=1}^lb_n(x_i)\\left(y_i - a_{n-1}(x_i)\\right)}{\\sum\\limits_{i=1}^lb_n^2(x_i)}\n",
    "$$\n",
    "Let's also find the second derivative:\n",
    "$$\n",
    "\\frac{\\partial^2 G_n\\left(b_n, \\gamma\\right)}{\\partial \\gamma^2} = \\sum\\limits_{i=1}^lb_n^2(x_i) \\geq 0\n",
    "$$\n",
    "Therefore, we obtained the optimal value of $\\gamma_n$ for MSE loss function:\n",
    "$$\n",
    "\\gamma_n = \\frac{\\sum\\limits_{i=1}^lb_n(x_i)\\left(y_i - a_{n-1}(x_i)\\right)}{\\sum\\limits_{i=1}^lb_n^2(x_i)}\n",
    "$$\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (1+1+1 pt.)\n",
    "\n",
    "Let $S = (x_i, y_i)_{i=1}^l$ be a sample for a classification task $y_i \\in \\{-1, +1\\}$.\n",
    "\n",
    "The **AdaBoost** algorithm can be regarded as a gradient boosting algorithm\n",
    "with the exponential loss $L(y,z) = e^{-y z}$. Consider the state of **AdaBoost**\n",
    "at the $T$-step\n",
    "$$ G_{T}(b, \\gamma)\n",
    "    = \\sum_{i=1}^l L\\bigl(y_i, a_{T-1}(x_i) + \\gamma b(x_i)\\bigr)\n",
    "    = \\sum_{i=1}^l \\underbrace{\\exp(- y_i a_{T-1}(x_i))}_{w^{T-1}_i}\n",
    "        \\exp(- y_i \\gamma b(x_i))\n",
    "    \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.1 (1 pt.)\n",
    "\n",
    "Derive the next weights $w^T_i$ used in $G_T$ at the stage $T$ as a function\n",
    "of the learnt classifier $b_T$ and the current weights $w^{T-1}_i$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "We know that:\n",
    "$$\n",
    "a_T(x_i) = a_{T-1}(x_i) + \\gamma_Tb_T(x_i)\n",
    "$$\n",
    "Let's multiply two sides by $-y_i$ and take the exponent:\n",
    "$$\n",
    "e^{-y_ia_T(x_i)} = e^{-y_ia_{T-1}(x_i)}e^{-y_i\\gamma_T b_T(x_i)} \\Rightarrow w_i^T = w_i^{T - 1}e^{-y_i\\gamma_T b_T(x_i)}\n",
    "$$\n",
    "Now let's find $\\gamma_T$. Taking into the account that we have a binary classification task with $y_i \\in \\{-1, 1\\}$, we may obtain:\n",
    "$$\n",
    "G_T\\left(b_T, \\gamma\\right) = \\sum\\limits_{i=1}^lw_i^{T-1}e^{-y_i\\gamma b_T(x_i)} = \\sum\\limits_{i=1}^lw_i^{T-1}\\left(e^\\gamma I\\{b_T(x_i) \\neq y_i\\} + e^{-\\gamma}I\\{b_T(x_i) = y_i\\}\\right)\n",
    "$$\n",
    "Taking the derivative and making it equal to zero, we get:\n",
    "$$\n",
    "\\frac{\\partial G_T\\left(b_T, \\gamma\\right)}{\\partial \\gamma} = e^\\gamma\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) \\neq y_i\\} - e^{-\\gamma}\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) = y_i\\} = 0 \\Rightarrow e^{2\\gamma}\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) \\neq y_i\\} = \\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) = y_i\\} \\Rightarrow \\\\\n",
    "\\Rightarrow e^{2\\gamma} = \\frac{\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) = y_i\\}}{\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) \\neq y_i\\}} \\Rightarrow \\gamma = \\frac{1}{2}\\ln\\left(\\frac{\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) = y_i\\}}{\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) \\neq y_i\\}}\\right)\n",
    "$$\n",
    "Let's take the second derivative to ensure that this is a point of minimum:\n",
    "$$\n",
    "\\frac{\\partial^2 G_T\\left(b_T, \\gamma\\right)}{\\partial \\gamma^2} = e^\\gamma\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) \\neq y_i\\} + e^{-\\gamma}\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) = y_i\\} \\geq 0,\n",
    "$$\n",
    "because $w_i^{T-1}$ are the exponents. Substituting the obtained value of $\\gamma_T$ to the expression for $w_i^T$, we can finally get:\n",
    "$$\n",
    "w_i^T = w_i^{T - 1}e^{-y_i\\gamma_T b_T(x_i)} = w_i^{T - 1}e^{-\\frac{y_i}{2}\\ln\\left(\\frac{\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) = y_i\\}}{\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) \\neq y_i\\}}\\right) b_T(x_i)} = w_i^{T - 1}\\left(\\frac{\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) = y_i\\}}{\\sum\\limits_{i=1}^lw_i^{T-1}I\\{b_T(x_i) \\neq y_i\\}}\\right)^{-\\frac{y_i}{2}b_T(x_i)}\n",
    "$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2 (1 pt.)\n",
    "\n",
    "Compute the ratio of weights $(w^T_i)_{i=1}^l$ between the normal and outlier\n",
    "samples in $S$. Propose a **formal definition of being an outlier**, and reflect\n",
    "on the value of *margin* for both.\n",
    "\n",
    "<span style=\"color:green\">**HINT**</span>: *margin* value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "Let's take a weight of outlier sample $w_O^T$ and weight of normal sample $w_N^T$ and look at the ratio between them:\n",
    "\n",
    "$$\n",
    "\\frac{w_O^T}{w_N^T} = \\frac{e^{-y_Oa_{T-1}\\left(x_O\\right)}}{e^{-y_Na_{T-1}\\left(x_N\\right)}} = e^{y_Na_{T-1}\\left(x_N\\right) - y_Oa_{T-1}\\left(x_O\\right)}\n",
    "$$\n",
    "\n",
    "Outlier is an object which doesn't match the distribution of the sample. So naturally it should have had another target value, but for some reason (mistake, etc.) it has the one it has. So a classifier may naturally confidently put it in one class when actually it belongs to the other one. This confidence might result in pretty large absolute value of margin $y_Oa_{T-1}\\left(x_O\\right)$, but with negative sign because prediction and target differ in case of outlier. On the other hand, a good classifier will either put the normal sample in a correct class (probably with big confidence) or make a mistake on it, but won't be sure at all. So, in case of normal sample the margin $y_Na_{T-1}\\left(x_N\\right)$ will either be positive or small negative. Therefore $e^{y_Na_{T-1}\\left(x_N\\right) - y_Oa_{T-1}\\left(x_O\\right)}$ may become relatively large, which means that $w_O^T$ is much bigger than $w_N^T$.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3 (1 pt.)\n",
    "\n",
    "Conclude about **sensitivity** of Adaboost to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "As was shown in 3.2, outlier samples have way bigger weights than normal samples. Therefore in case of making a mistake on an outlier, a sample which should ideally have had another target value would cause a large penalty on the classifier, which may significantly change a behaviour of forming a decision boundary, and thus have a large impact on the construction of the final model. It implies that Adaboost is very sensitive to outliers.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (2+1+2 pt.): Alternative objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem studies boosting-type algorithms defined with objective\n",
    "functions different from that of AdaBoost.We assume that the training\n",
    "data are given as m labeled examples $(x_{1},y_{1}),...,(x_{m},y_{m}) \\in X \\times \\{-1,+1\\}$.We further assume that $\\Phi$ is a strictly increasing convex and differentiable function over $\\mathbb{R}$ such that:$\\ \\forall x \\geqslant 0,\\Phi(x)\\geqslant 1 \\ and \\ \\forall x < 0,\\Phi(x) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1 (2 pt.)\n",
    "Consider the loss function $L(a) =\\sum_{i=1}^{m}\\Phi \\left( -y_{i}g(x_i) \\right)$ where $g$ is a linear combination of base classifiers, i.e., $g= \\sum_{t=1}^{T} a_t h_t$(as in\n",
    "AdaBoost). Derive a new boosting algorithm using the objective function $L$. In particular, characterize the best base classifier $h_u$ to select at each round of boosting if we use coordinate descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "Using coordinate descent in the space of algorithms, we can obtain the optimal classifier $h_u$ for the next step:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "h_u = \\arg\\min_h\\frac{\\partial L\\left(g + \\gamma h\\right)}{\\partial \\gamma}\\Big|_{\\gamma = 0} =\\\\\n",
    "&&= \\arg\\min_h\\frac{\\partial \\sum\\limits_{i=1}^m\\Phi\\left(-y_i\\left(g(x_i) + \\gamma h(x_i)\\right)\\right)}{\\partial \\gamma}\\Big|_{\\gamma = 0} =\\\\\n",
    "&&= \\arg\\min_h\\left(-\\sum\\limits_{i=1}^my_ih(x_i)\\Phi'\\left(-y_ig(x_i)\\right)\\right) =\\\\\n",
    "&&= \\left[w_i^T = \\Phi'\\left(-y_ig(x_i)\\right)\\right] =\\\\\n",
    "&&= \\arg\\min_h\\left(-\\sum\\limits_{i=1}^mw_i^Ty_ih(x_i)\\right) =\\\\\n",
    "&&= \\arg\\min_h\\left(-\\sum\\limits_{i=1}^mw_i^TI\\{h(x_i) = y_i\\} + \\sum\\limits_{i=1}^mw_i^TI\\{h(x_i) \\neq y_i\\}\\right) =\\\\\n",
    "&&= \\arg\\min_h\\sum\\limits_{i=1}^mw_i^T\\left(I\\{h(x_i) \\neq y_i\\} - I\\{h(x_i) = y_i\\}\\right) =\\\\\n",
    "&&= \\arg\\min_h\\sum\\limits_{i=1}^mw_i^T\\left(2I\\{h(x_i) \\neq y_i\\} - 1\\right) =\\\\\n",
    "&&= \\arg\\min_h\\sum\\limits_{i=1}^mw_i^TI\\{h(x_i) \\neq y_i\\}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Then let's try to find optimal $\\gamma_u$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\gamma_u = \\arg\\min_\\gamma G_T\\left(h_u, \\gamma\\right) =\\\\\n",
    "&&= \\arg\\min_\\gamma \\sum\\limits_{i=1}^m\\Phi\\left(-y_i\\left(g(x_i) + \\gamma h_u(x_i)\\right)\\right) \\Rightarrow\\\\\n",
    "&&\\Rightarrow \\left(\\sum\\limits_{i=1}^m\\Phi\\left(-y_i\\left(g(x_i) + \\gamma_u h_u(x_i)\\right)\\right)\\right)' = 0 \\Rightarrow\\\\\n",
    "&&\\Rightarrow \\sum\\limits_{i=1}^my_ih_u\\left(x_i\\right)\\Phi'\\left(y_i\\left(g(x_i) + \\gamma_uh_u(x_i)\\right)\\right) = 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "Therefore, $\\gamma_u$ satisfies the equation above and depends on how $\\Phi$ looks like.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2 (1 pt.)\n",
    "Consider the following functions:  \n",
    "\n",
    "1. zero-one loss $\\Phi_1(−u) = 1_{u\\leqslant0}$;  \n",
    "2. least squared loss $\\Phi_2(−u) = (1 − u)^2$;  \n",
    "3. SVM loss $\\Phi_3(−u) = \\max\\{0, 1 − u\\}$;  \n",
    "4. logistic loss $\\Phi_4(−u) = \\log(1 + e^{−u})$.  \n",
    "\n",
    "Which functions satisfy the assumptions on $\\Phi$ stated earlier in this\n",
    "problem?\n",
    "\n",
    "Compute the gradient for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "Functions 1-3 doesn't satisfy the assumptions because they are not **strictly** increasing, also it is not true that for $-u < 0$ $\\Phi(-u) > 0$. However, function 4 satisfy all the assumptions: it is strictly increasing, for $-u \\geq 0$ $\\Phi_4(-u) \\geq 1$ (assuming $\\log$ is $\\log_2$) and for $-u < 0$ $\\Phi_4(-u) > 0$. Let's find its derivative (gradient):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\Phi_4(-u)}{\\partial(-u)} = \\frac{e^{-u}}{\\left(1 + e^{-u}\\right)\\ln2}\n",
    "$$\n",
    "\n",
    "It exists everywhere, so $\\Phi_4(-u)$ is differentiable. Let's also check that it is convex, taking the second derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 \\Phi_4(-u)}{\\partial(-u)^2} = \\frac{e^{-u}\\left(1 + e^{-u}\\right) - \\left(e^{-u}\\right)^2}{\\left(1 + e^{-u}\\right)^2\\ln2} = \\frac{e^{-u}}{\\left(1 + e^{-u}\\right)^2\\ln2} \\geq 0\\qquad\\text{for any }u\n",
    "$$\n",
    "\n",
    "So, it is convex. Thus, $\\Phi_4(-u)$ is the only function among the given four which satisfies all the assumptions. The derivative was computed above.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3* (2 pt.)\n",
    "For each loss function satisfying the assumptions in Task 5 formualtion, derive the\n",
    "corresponding boosting algorithm. How do the algorithm(s) differ\n",
    "from AdaBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "Substituting $\\Phi'(-u) = \\Phi_4'(-u) = \\frac{e^{-u}}{\\left(1 + e^{-u}\\right)\\ln2}$ to the derived equations in Task 4.1, we obtain:\n",
    "\n",
    "$$\n",
    "h_u = \\arg\\min_h\\sum\\limits_{i=1}^m\\Phi'\\left(-y_ig(x_i)\\right)I\\{h(x_i) \\neq y_i\\} = \\arg\\min_h\\sum\\limits_{i=1}^m\\frac{e^{-y_ig(x_i)}}{1 + e^{-y_ig(x_i)}}I\\{h(x_i) \\neq y_i\\} = \\arg\\min_h\\sum\\limits_{i=1}^m\\frac{I\\{h(x_i) \\neq y_i\\}}{1 + e^{y_ig(x_i)}}\n",
    "$$\n",
    "\n",
    "Assuming that we obtained $h_u$, then we can get $\\gamma_u$:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\sum\\limits_{i=1}^my_ih_u\\left(x_i\\right)\\Phi'\\left(y_i\\left(g(x_i) + \\gamma_uh_u(x_i)\\right)\\right) = 0 \\Rightarrow\\\\\n",
    "&&\\Rightarrow \\sum\\limits_{i=1}^my_ih_u\\left(x_i\\right)\\frac{e^{-y_i\\left(g(x_i) + \\gamma_uh_u(x_i)\\right)}}{\\left(1 + e^{-y_i\\left(g(x_i) + \\gamma_uh_u(x_i)\\right)}\\right)\\ln2} = 0 \\Rightarrow\\\\\n",
    "&&\\Rightarrow \\sum\\limits_{i=1}^my_ih_u\\left(x_i\\right)\\frac{1}{1 + e^{y_i\\left(g(x_i) + \\gamma_uh_u(x_i)\\right)}} = 0 \\Rightarrow\\\\\n",
    "&&\\Rightarrow \\sum\\limits_{i=1}^m\\left(\\frac{I\\{h_u(x_i) = y_i\\}}{1 + e^{y_ig(x_i) + \\gamma_n}} - \\frac{I\\{h_u(x_i) \\neq y_i\\}}{1 + e^{y_ig(x_i) - \\gamma_n}}\\right) = 0\\Rightarrow\\\\\n",
    "&&\\Rightarrow \\sum\\limits_{i=1}^me^{y_ig(x_i)}\\left(e^{\\gamma_n}I\\{h_u(x_i) \\neq y_i\\} - e^{-\\gamma_n}I\\{h_u(x_i) = y_i\\}\\right) = \\sum\\limits_{i=1}^m\\left(I\\{h_u(x_i) = y_i\\} - I\\{h_u(x_i) \\neq y_i\\}\\right) \\Rightarrow\\\\\n",
    "&&\\Rightarrow e^{2\\gamma_n}\\sum\\limits_{i=1}^me^{y_ig(x_i)}I\\{h_u(x_i) \\neq y_i\\} - e^{\\gamma_n}\\sum\\limits_{i=1}^m\\left(I\\{h_u(x_i) = y_i\\} - I\\{h_u(x_i \\neq y_i\\}\\right) - \\sum\\limits_{i=1}^me^{e^{y_i}g(x_i)}I\\{h_u(x_i) = y_i\\} = 0,\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "which is a quadratic equation on $e^{\\gamma_n}$ and can be solved based on the form of $h_u$.\n",
    "\n",
    "As we can see, the algorithm developed is similar to Adaboost, the only thing which differs is the weights $w_i^T$ because we only changed $\\Phi$, and it appears only in them.\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. (1 pt.)\n",
    "Consider a two-layer network function of the form\n",
    "    \\begin{equation}\n",
    "    y_k(x, \\mathbf{w}) = \\sigma \\left ( \\sum_{j=1}^M w_{kj}^{(2)} \\sigma \\left(\\sum_{i=1}^D w_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right)\n",
    "                               + w_{k0}^{(2)}\\right)\n",
    "    \\end{equation}\n",
    "in which the nonlinear activation functions are logistic sigmoid functions\n",
    "    \\begin{equation}\n",
    "    \\sigma(a) = (1 + \\exp(−a))^{-1}.\n",
    "    \\end{equation}\n",
    "Show that there exists an equivalent network, which computes exactly the same function but with hidden unit activation function given by hyperbolic tangent ${\\rm tanh}(a)$\n",
    "    \\begin{equation}\n",
    "    {\\rm tanh}(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "Notice that:\n",
    "\n",
    "$$\n",
    "{\\rm tanh}(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}} = \\frac{1 - e^{-2a}}{1 + e^{-2a}} = \\left(1 - e^{-2a}\\right)\\sigma(2a) = \\sigma(2a) - \\left(1 - \\sigma(2a)\\right) = 2\\sigma(2a) - 1\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\sigma(2a) = \\frac{{\\rm tanh}(a) + 1}{2} \\Rightarrow \\sigma(a) = \\frac{1}{2}{\\rm tanh}\\left(\\frac{a}{2}\\right) + \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "Substituting this to the network function instead of the hidden unit activation function, we obtain:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y_k(x, \\mathbf{w}) =\\\\\n",
    "&&= \\sigma\\left(\\sum_{j=1}^Mw_{kj}^{(2)}\\sigma\\left(\\sum_{i=1}^Dw_{ji}^{(1)}x_i + w_{j0}^{(1)}\\right) + w_{k0}^{(2)}\\right) =\\\\\n",
    "&&= \\sigma\\left(\\sum_{j=1}^M\\frac{w_{kj}^{(2)}}{2}{\\rm tanh}\\left(\\sum_{i=1}^D\\frac{w_{ji}^{(1)}}{2}x_i + \\frac{w_{j0}^{(1)}}{2}\\right) + \\sum_{j=1}^M\\frac{w_{kj}^{(2)}}{2} + w_{k0}^{(2)}\\right) =\\\\\n",
    "&&= \\sigma\\left(\\sum_{j=1}^Mw_{kj}^{(2)'}{\\rm tanh}\\left(\\sum_{i=1}^Dw_{ji}^{(1)'}x_i + w_{j0}^{(1)'}\\right) + w_{k0}^{(2)'}\\right),\n",
    "\\end{eqnarray}\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "w_{j0}^{(1)'} = \\frac{1}{2}w_{j0}^{(1)}\n",
    "$$\n",
    "$$\n",
    "w_{ji}^{(1)'} = \\frac{1}{2}w_{ji}^{(1)}\n",
    "$$\n",
    "$$\n",
    "w_{k0}^{(2)'} = \\sum\\limits_{j=1}^M\\frac{w_{kj}^{(2)}}{2} + w_{k0}^{(2)}\n",
    "$$\n",
    "$$\n",
    "w_{kj}^{(2)'} = \\frac{w_{kj}^{(2)}}{2}\n",
    "$$\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6*. Data augmentation (2 pt.)\n",
    "One way to encourage invariance of a model w.r.t a set of transformations is to expand the training set using transformed versions of the original input patterns.\n",
    "Consider the framework for training with transformed data in the special case when the transformation is simply addition of random noise $x \\rightarrow x + \\xi$ where $\\xi$ has a Gaussian distribution with zero mean and unit variance. The error function for untransformed inputs can be written (in the infinite data set limit) in the form\n",
    "    \\begin{equation}\n",
    "    E = \\frac12 \\int \\int (y(\\mathbf{x}) - t)^2 p(t|\\mathbf{x}) p(\\mathbf{x}){\\rm d}\\mathbf{x} {\\rm d}t.\n",
    "    \\end{equation}\n",
    "If we now consider an infinite number of copies of each data point perturbed by the transformation, then the error function can be written as\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} = \\frac12 \\int\\int(y(x + \\xi) - t)^2p(t | \\mathbf{x})p(\\mathbf{x}) p(\\xi){\\rm d}\\mathbf{x}{\\rm d}t {\\rm d}\\xi\n",
    "    \\end{equation}\n",
    "Using Taylor series expansion of $y(\\mathbf{x} + \\xi)$ show that\n",
    "    \\begin{equation}\n",
    "    \\widetilde{E} \\simeq E + \\lambda \\Omega\n",
    "    \\end{equation}\n",
    "where $\\Omega$ is a regularizer which takes the form of Tikhonov regularizer\n",
    "    \\begin{equation}\n",
    "    \\Omega = \\frac12 \\int \\|\\nabla y(\\mathbf{x})\\|^2 p(\\mathbf{x}){\\rm d} \\mathbf{x}.\n",
    "    \\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEGIN Solution**\n",
    "\n",
    "Let's obtain a representation of $y(x + \\xi)$ using second-order Taylor series expansion of a multi-variable function:\n",
    "\n",
    "$$\n",
    "y(x + \\xi) \\simeq y(x) + \\left(\\nabla y(x)\\right)^T\\xi + \\frac{1}{2}\\xi^T\\left(\\nabla^2 y(x)\\right)\\xi\n",
    "$$\n",
    "\n",
    "Subtstituting it to the expression for $\\widetilde{E}$, we get:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\widetilde{E} \\simeq\\\\\n",
    "&&\\simeq \\frac12 \\int\\int\\int\\left(y(x) + \\left(\\nabla y(x)\\right)^T\\xi + \\frac{1}{2}\\xi^T\\left(\\nabla^2 y(x)\\right)\\xi - t\\right)^2p(t | x)p(x) p(\\xi){\\rm d}x{\\rm d}t {\\rm d}\\xi \\simeq\\\\\n",
    "&&\\simeq \\frac12 \\int\\int\\int (y(x) - t)^2 p(t|x) p(x){\\rm d}x {\\rm d}t + \\frac12 \\int\\int\\int\\left(\\left(\\nabla y(x)\\right)^T\\xi + \\frac{1}{2}\\xi^T\\left(\\nabla^2 y(x)\\right)\\xi\\right)^2p(t | x)p(x) p(\\xi){\\rm d}x{\\rm d}t {\\rm d}\\xi +\\\\\n",
    "&&+ \\int\\int\\int(y(x) - t)\\left(\\left(\\nabla y(x)\\right)^T\\xi + \\frac{1}{2}\\xi^T\\left(\\nabla^2 y(x)\\right)\\xi\\right)p(x) p(\\xi){\\rm d}x{\\rm d}t {\\rm d}\\xi \\simeq\\\\\n",
    "&&\\simeq E + \\frac12 \\int\\int\\int\\left(\\left(\\nabla y(x)\\right)^T\\xi\\right)^2p(t | x)p(x) p(\\xi){\\rm d}x{\\rm d}t {\\rm d}\\xi + \\int\\int\\int(y(x) - t)\\left(\\left(\\nabla y(x)\\right)^T\\xi + \\frac{1}{2}\\xi^T\\left(\\nabla^2 y(x)\\right)\\xi\\right)p(x) p(\\xi){\\rm d}x{\\rm d}t {\\rm d}\\xi \\simeq\\\\\n",
    "&&\\simeq E + \\frac12\\int\\int\\int\\left(\\left(\\nabla y(x)\\right)^T\\xi\\right)^2p(t | x)p(x)p(\\xi){\\rm d}x{\\rm d}t{\\rm d}\\xi + \\int\\int(y(x) - t)\\left(\\nabla y(x)\\right)^T\\mathbb{E}[\\xi] p(t | x)p(x){\\rm d}x{\\rm d}t{\\rm d}\\xi +\\\\\n",
    "&&+ \\frac12\\int\\int\\int(y(x) - t)\\xi^T\\left(\\nabla^2y(x)\\right)\\xi p(t | x)p(x)p(\\xi){\\rm d}x{\\rm d}t{\\rm d}\\xi \\simeq\\\\\n",
    "&&\\simeq E + \\frac12\\int\\int\\left(\\left(\\nabla y(x)\\right)^T\\xi\\right)^2p(x)p(\\xi){\\rm d}x{\\rm d}\\xi + \\frac12\\int\\int(y(x) - \\mathbb{E}\\left[t | x\\right])\\xi^T\\left(\\nabla^2y(x)\\right)\\xi p(x)p(\\xi){\\rm d}x{\\rm d}\\xi\n",
    "\\end{eqnarray}\n",
    "\n",
    "Since we are looking for the optimal $y(x)$, we can take a derivative of loss with respect to it and make it equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial y(x)} = \\int\\left(y(x) - t\\right)p(t|x)p(x){\\rm d}t = 0 \\Rightarrow y(x) = \\int tp(t|x){\\rm d}t = \\mathbb{E}\\left[t | x\\right]\n",
    "$$\n",
    "\n",
    "Substituting this to the expression above, we finally obtain:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\widetilde{E} \\simeq\\\\\n",
    "&&\\simeq E + \\frac12\\int\\int\\left(\\left(\\nabla y(x)\\right)^T\\xi\\right)^2p(x)p(\\xi){\\rm d}x{\\rm d}\\xi\\simeq\\\\\n",
    "&&\\simeq E + \\frac12\\int\\int\\xi^T\\left(\\nabla y(x)\\right)\\left(\\nabla y(x)\\right)^T\\xi p(x)p(\\xi){\\rm d}x{\\rm d}\\xi\\simeq\\\\\n",
    "&&\\simeq E + \\frac12\\int\\int{\\rm tr}\\left(\\left(\\left(\\nabla y(x)\\right)\\left(\\nabla y(x)\\right)^T\\right)\\left(\\xi\\xi^T\\right)\\right) p(x)p(\\xi){\\rm d}x{\\rm d}\\xi\\simeq\\\\\n",
    "&&\\simeq E + \\frac12\\int\\int{\\rm tr}\\left(\\left(\\nabla y(x)\\right)\\left(\\nabla y(x)\\right)^T\\right) p(x)p(\\xi){\\rm d}x{\\rm d}\\xi\\simeq\\\\\n",
    "&&\\simeq E + \\frac12\\int\\left(\\nabla y(x)\\right)^T\\left(\\nabla y(x)\\right) p(x){\\rm d}x\\simeq\\\\\n",
    "&&\\simeq E + \\frac12\\int\\|\\nabla y(x)\\|^2 p(x){\\rm d}x\\simeq\\\\\n",
    "&&\\simeq E + \\Omega,\\qquad\\text{q.e.d.}\n",
    "\\end{eqnarray}\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
