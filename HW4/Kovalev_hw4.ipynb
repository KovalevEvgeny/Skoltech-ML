{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 4 (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your implementation within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```\n",
    "\n",
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly / outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will need to work through a modified version of\n",
    "the SVDD model (**support vector data description**) -- a model for outlier\n",
    "detection, and show some theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we have a dataset $x_1, \\ldots, x_l$ from some set $\\mathcal{X}$.\n",
    "\n",
    "We apply the kernel trick using the kernel $K \\colon \\mathcal{X} \\times \\mathcal{X}\n",
    "\\to \\mathbb{R}$ of the Hilbert space $\\bigl(\\mathcal{H}, \\langle \\cdot,\n",
    "\\cdot \\rangle\\big)$ with the feature mapping $\\phi(\\cdot)\\colon \\mathcal{X}\n",
    "\\to \\mathcal{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that $\\nu \\in (0, 1)$, and $C_i > 0$ with $\\sum_{i=1}^l C_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVDD model (kernelized) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\underset{R, h, \\xi}{\\text{minimize}}\n",
    "      & & R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i\n",
    "          \\,, \\\\\n",
    "    & \\text{subject to}\n",
    "      & & \\|\\phi(x_i) - h \\|^2 \\leq R + \\xi_i\n",
    "          \\,, \\\\\n",
    "    & & & \\xi_i \\geq 0\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (3 pt.): Can $R$ be negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the SVDD problem with an extra constraint $R \\geq 0$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\underset{R, h, \\xi}{\\text{minimize}}\n",
    "      & & R + \\frac1\\nu \\sum_{i=1}^l C_i \\xi_i\n",
    "          \\,, \\\\\n",
    "    & \\text{subject to}\n",
    "      & & \\|\\phi(x_i) - h \\|^2 \\leq R + \\xi_i\n",
    "          \\,, \\\\\n",
    "    & & & \\xi_i \\geq 0\n",
    "          \\,, \\\\\n",
    "    & & & R \\geq 0\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R \\geq 0$ constraint is a nuisance.\n",
    "\n",
    "* Show, that if $(R, \\xi, h)$ has $R < 0$, then it **is not better** than a\n",
    "certain other feasible candidate $(\\hat{R}, \\hat{\\xi}, \\hat{h})$ with $\\hat{R} \\geq 0$.\n",
    "* Argue that it is, in fact, **redundant**, i.e. the problem formulations\n",
    "**with it** and **without it** have identical solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) Denote\n",
    "\n",
    "$$\n",
    "f(R, h, \\xi) = R + \\frac{1}{\\nu}\\sum\\limits_{i=1}^\\ell C_i\\xi_i\n",
    "$$\n",
    "\n",
    "Consider $\\hat{R} = -R > 0$, $\\hat{\\xi}_i = \\xi_i + 2R$, $\\hat{h} = h$. Then the constraints are the same (because $\\hat{R} + \\hat{\\xi}_i = R + \\xi_i$). Let's take a look at the objective:\n",
    "\n",
    "$$\n",
    "f(\\hat{R}, \\hat{h}, \\hat{\\xi}) = -R + \\frac{1}{\\nu}\\sum\\limits_{i=1}^\\ell C_i(\\xi_i + 2R) = -R + \\frac{1}{\\nu}\\sum\\limits_{i=1}^\\ell C_i\\xi_i + \\frac{2R}{\\nu}\\sum\\limits_{i=1}^\\ell C_i = -R + \\frac{1}{\\nu}\\sum\\limits_{i=1}^\\ell C_i\\xi_i + \\frac{2R}{\\nu},\n",
    "$$\n",
    "\n",
    "since $\\sum\\limits_{i=1}^\\ell C_i = 1$. Now let's subtract one from the other:\n",
    "\n",
    "$$\n",
    "f(R, h, \\xi) - f(\\hat{R}, \\hat{h}, \\hat{\\xi}) = 2R - \\frac{2R}{\\nu} \\geq 0,\n",
    "$$\n",
    "\n",
    "since $\\nu \\in (0, 1)$ and $R < 0$. Therefore the objective is smaller for $(\\hat{R}, \\hat{\\xi}, \\hat{h})$, and since we want to minimize it, it implies that a candidate $(R, \\xi, h)$ with $R < 0$ is not the best.\n",
    "\n",
    "2) It follows from the reasonings above that the constraint $R \\geq 0$ is redundant, because a solution to the problem will never have $R < 0$ (otherwise there would have been a better solution with $R \\geq 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (2 pt.): Can $\\xi_i > 0$ for all $i$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argue if $(R, \\xi, h)$ is a solution, then $\\xi_i = 0$ for at least one $i=1,\\,\\ldots,\\,l$. Let $\\bar{\\xi} = \\min_{j=1}^l \\xi_j$.\n",
    "\n",
    "**HINT** Use an argument similar to Task $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Suppose that $\\xi_i > 0$ for all $i$. Let's pick $\\hat{\\xi}_i = \\xi_i - \\bar{\\xi}$ (then one of $\\xi_i = 0$ because $\\bar{\\xi} - \\bar{\\xi} = 0$), $\\hat{R} = R + \\bar{\\xi}$ so that $\\hat{R} + \\hat{\\xi}_i = R + \\xi_i$ and $\\hat{h} = h$. Then:\n",
    "\n",
    "$$\n",
    "f(\\hat{R}, \\hat{h}, \\hat{\\xi}) = R + \\bar{\\xi} + \\frac{1}{\\nu}\\sum\\limits_{i=1}^\\ell C_i\\xi_i - \\frac{\\bar{\\xi}}{\\nu}\n",
    "$$\n",
    "\n",
    "Subtracting this from the initial objective function, we obtain:\n",
    "\n",
    "$$\n",
    "f(R, h, \\xi) - f(\\hat{R}, \\hat{h}, \\hat{\\xi}) = \\frac{\\bar{\\xi}}{\\nu} - \\bar{\\xi} \\geq 0, \n",
    "$$\n",
    "\n",
    "and analogically with the previous task we have that the solution with all $\\xi_i > 0$ is not the best. Hence, $\\xi_i = 0$ for at least one $i = 1, \\ldots, \\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (2 pt.): The Lagrangian and KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write out the Lagrangian function of the problem and write out the KKT conditions\n",
    "* Lagrangian\n",
    "* the first order conditions\n",
    "* the complementary slackness conditions\n",
    "* the primal and dual constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lagrangian:\n",
    "\n",
    "$$\n",
    "L(R, h, \\xi, \\lambda, \\mu) = R + \\frac{1}{\\nu}\\sum\\limits_{i=1}^\\ell C_i\\xi_i + \\sum\\limits_{i=1}^\\ell\\alpha_i(\\|\\phi(x_i) - h\\|^2 - R - \\xi_i) - \\sum\\limits_{i=1}^\\ell\\beta_i\\xi_i\n",
    "$$\n",
    "\n",
    "First order conditions:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial R} = 0 \\Rightarrow \\sum\\limits_{i=1}^\\ell\\alpha_i = 1 \\qquad\\qquad(1)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h} = 0 \\Rightarrow \\sum\\limits_{i=1}^\\ell\\alpha_i(h - \\phi(x_i)) = 0 \\Rightarrow h = \\frac{\\sum\\limits_{i=1}^\\ell\\alpha_i\\phi(x_i)}{\\sum\\limits_{i=1}^\\ell\\alpha_i} = \\sum\\limits_{i=1}^\\ell\\alpha_i\\phi(x_i) \\qquad\\qquad(2)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = 0 \\Rightarrow \\frac{C_i}{\\nu} - \\alpha_i - \\beta_i = 0 \\qquad\\qquad(3)\n",
    "$$\n",
    "\n",
    "Complementary slackness conditions:\n",
    "\n",
    "$$\n",
    "\\alpha_i(\\|\\phi(x_i) - h\\|^2 - R - \\xi_i) = 0\n",
    "$$\n",
    "$$\n",
    "\\beta_i\\xi_i = 0\n",
    "$$\n",
    "\n",
    "Primal constraints:\n",
    "\n",
    "$$\n",
    "\\|\\phi(x_i) - h\\|^2 - R - \\xi_i \\leq 0\n",
    "$$\n",
    "$$\n",
    "-\\xi_i \\leq 0\n",
    "$$\n",
    "\n",
    "Dual constraints:\n",
    "\n",
    "$$\n",
    "\\alpha_i \\geq 0\n",
    "$$\n",
    "$$\n",
    "\\beta_i \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (2 pt.): Simplifying the Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down $h$ as a function of transformed input data and dual coefficients,\n",
    "and using the first order conditions rewrite the Lagrangian in such a way, that\n",
    "it only contains dual variables and evaluations of the kernel $K(\\cdot, \\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It was derived above that:\n",
    "\n",
    "$$\n",
    "h = \\sum\\limits_{i=1}^\\ell\\alpha_i\\phi(x_i)\n",
    "$$\n",
    "\n",
    "Let's substitute $(1)$ into the Lagrangian:\n",
    "\n",
    "$$\n",
    "L(R, h, \\xi, \\alpha, \\beta) = \\frac{1}{\\nu}\\sum\\limits_{i=1}^\\ell C_i\\xi_i + \\sum\\limits_{i=1}^\\ell\\alpha_i(\\|\\phi(x_i) - h\\|^2 - \\xi_i) - \\sum\\limits_{i=1}^\\ell\\beta_i\\xi_i\n",
    "$$\n",
    "\n",
    "Let's substitute $(3)$:\n",
    "\n",
    "$$\n",
    "L(R, h, \\xi, \\alpha, \\beta) = \\sum\\limits_{i=1}^\\ell \\left(\\frac{C_i}{\\nu} - \\alpha_i - \\beta_i\\right)\\xi_i + \\sum\\limits_{i=1}^\\ell\\alpha_i\\|\\phi(x_i) - h\\|^2 = \\sum\\limits_{i=1}^\\ell\\alpha_i\\|\\phi(x_i) - h\\|^2\n",
    "$$\n",
    "\n",
    "Finally, let's substitute $(2)$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L(R, h, \\xi, \\alpha, \\beta) =\\\\\n",
    "&&= \\sum\\limits_{i=1}^\\ell\\alpha_i\\|\\phi(x_i) - \\sum\\limits_{j=1}^\\ell\\alpha_j\\phi(x_j)\\|^2 =\\\\\n",
    "&&= \\sum\\limits_{i=1}^\\ell\\alpha_i\\phi(x_i)^2 - 2\\sum\\limits_{i,j=1}^\\ell\\alpha_i\\alpha_j\\phi(x_i)\\phi(x_j) + \\sum\\limits_{i=1}^\\ell\\alpha_i\\left(\\sum\\limits_{j=1}^\\ell \\alpha_j\\phi(x_j)\\right)^2 =\\\\\n",
    "&&= \\sum\\limits_{i=1}^\\ell\\alpha_iK(x_i, x_i) - 2\\sum\\limits_{i,j=1}^\\ell\\alpha_i\\alpha_jK(x_i, x_j) + \\sum\\limits_{i,j=1}^\\ell\\alpha_i\\alpha_jK(x_i, x_j) =\\\\\n",
    "&&= \\sum\\limits_{i=1}^\\ell\\alpha_iK(x_i, x_i) - \\sum\\limits_{i,j=1}^\\ell\\alpha_i\\alpha_jK(x_i, x_j)\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (2 pt.): The dual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carefully eliminate $\\beta_i$ from the KKT conditions, and write\n",
    "down the inequality constraints for the dual variables $\\alpha_i$,\n",
    "implied by the FOC.\n",
    "\n",
    "* Combine your results into dual problem (minimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) According to $(3)$, we have:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{C_i}{\\nu} - \\beta_i\n",
    "$$\n",
    "\n",
    "Notice that $\\alpha_i \\geq 0$ and $\\beta_i \\geq 0$, hence we may come up with the following constraints for $\\alpha_i$:\n",
    "\n",
    "$$\n",
    "0 \\leq \\alpha_i \\leq \\frac{C_i}{\\nu}\n",
    "$$\n",
    "\n",
    "2) So, we may obtain dual problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & \\underset{\\alpha_i}{\\text{minimize}}\n",
    "      & & \\sum\\limits_{i=1}^\\ell\\alpha_iK(x_i, x_i) - \\sum\\limits_{i,j=1}^\\ell\\alpha_i\\alpha_jK(x_i, x_j)\n",
    "          \\,, \\\\\n",
    "    & \\text{subject to}\n",
    "      & & 0 \\leq \\alpha_i \\leq \\frac{C_i}{\\nu}\n",
    "          \\,.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, you have solved the dual and have optimal $(\\alpha^*_i)_{i=1}^l$ and\n",
    "$ h^* = \\sum_{i=1}^l \\alpha^*_i \\phi(x_i)$.\n",
    "\n",
    "Let $M = \\{i\\colon \\alpha^*_i \\in \\left(0, \\tfrac{C_i}{\\nu}\\right)\\}$ and suppose $M \\neq \\emptyset$.\n",
    "\n",
    "* Derive the expression for the optimal value of $R$ from this and the\n",
    "complementary sclackness conditions.\n",
    "\n",
    "* Write out the decision function for an arbitrary $x\\in \\mathcal{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) According to the first complementary slackness condition:\n",
    "\n",
    "$$\n",
    "\\alpha_i(\\|\\phi(x_i) - h\\|^2 - R - \\xi_i) = 0\n",
    "$$\n",
    "\n",
    "Let's consider only $\\alpha_i$ with $i \\in M$, because in other cases $\\alpha_i = 0$ and the condition holds for each $R$. So, for any $\\alpha_i^*$ with $i \\in M$ we have that $\\alpha_i^* > 0$ by the definition of $M$, and thus:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\|\\phi(x_i) - h^*\\|^2 - R - \\xi_i = 0 \\Rightarrow R =\\\\\n",
    "&&= \\|\\phi(x_i) - \\sum\\limits_{j=1}^\\ell\\alpha_j^*\\phi(x_j)\\|^2 - \\xi_i =\\\\\n",
    "&&= \\|\\phi(x_i)\\|^2 - 2\\langle\\phi(x_i), \\sum\\limits_{j=1}^\\ell\\alpha_j^*\\phi(x_j)\\rangle + \\|\\sum\\limits_{j=1}^\\ell\\alpha_j^*\\phi(x_j)\\|^2 - \\xi_i=\\\\\n",
    "&&= K(x_i, x_i) - 2\\sum\\limits_{j=1}^\\ell\\alpha_j^*K(x_i, x_j) + \\sum\\limits_{j, k = 1}^\\ell\\alpha_j^*\\alpha_k^*K(x_j, x_k) - \\xi_i\n",
    "\\end{eqnarray}\n",
    "\n",
    "Notice that for any $\\alpha_i^*$ with $i \\in M$ we have a corresponding $\\beta_i^* > 0$ since $0 < \\alpha_i^* < \\frac{C_i}{\\nu}$ and $\\alpha_i^* = \\frac{C_i}{\\nu} - \\beta_i^*$. According to the second complementary slackness condition:\n",
    "\n",
    "$$\n",
    "\\beta_i\\xi_i = 0\n",
    "$$\n",
    "\n",
    "Hence in this case $\\xi_i = 0$. So, we found an optimal value of $R$, which is:\n",
    "\n",
    "$$\n",
    "R = K(x_i, x_i) - 2\\sum\\limits_{j=1}^\\ell\\alpha_j^*K(x_i, x_j) + \\sum\\limits_{j, k = 1}^\\ell\\alpha_j^*\\alpha_k^*K(x_j, x_k)\n",
    "$$\n",
    "\n",
    "for any $i \\in M$.\n",
    "\n",
    "2) The decision function for an arbitrary $x \\in \\mathcal{X}$:\n",
    "\n",
    "$$\n",
    "\\|\\phi(x) - h \\|^2 \\leq R,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "R = K(x, x) - 2\\sum\\limits_{j=1}^\\ell\\alpha_j^*K(x, x_j) + \\sum\\limits_{j, k = 1}^\\ell\\alpha_j^*\\alpha_k^*K(x_j, x_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.1 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that for some $x_i$ we have $\\|\\phi(x_i) - h\\|^2 < R$.\n",
    "We will call this point **inlier**.\n",
    "\n",
    "* What are the values of $\\alpha_i$ and $\\beta_i$ for such a point?\n",
    "* Show that $1-\\nu$ upper-bounds the sum of weights $C_i$ for the **inlier** points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) Recall the first complementary slackness condition:\n",
    "\n",
    "$$\n",
    "\\alpha_i(\\|\\phi(x_i) - h\\|^2 - R - \\xi_i) = 0\n",
    "$$\n",
    "\n",
    "If $\\|\\phi(x_i) - h\\|^2 < R$, then, since $\\xi_i \\geq 0$, it can only hold if $\\alpha_i = 0$, and since $\\alpha_i = \\frac{C_i}{\\nu} - \\beta_i$, we also obtain that $\\beta_i = \\frac{C_i}{\\nu}$.\n",
    "\n",
    "2) Shown in 7.2 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.2 (2 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that for some $x_i$ it holds that $\\|\\phi(x_i) - h \\|^2 > R$.\n",
    "Such points are called **outliers**.\n",
    "\n",
    "* What are the values of $\\alpha_i$ and $\\beta_i$ for these points?\n",
    "* Argue that the sum of weights $C_i$ for the **outliers** is upper bounded by $\\nu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1) Analogically as in 7.1, if $\\|\\phi(x_i) - h\\|^2 > R$, then due to the first complementary slackness condition $\\xi_i > 0$, and then according to the second complementary slackness condition $\\beta_i = 0$. Since $\\alpha_i = \\frac{C_i}{\\nu} - \\beta_i$, we obtain that $\\alpha_i = \\frac{C_i}{\\nu}$.\n",
    "\n",
    "2) Denote as $\\alpha_i^B$, $\\alpha_i^O$, $\\alpha_i^I$ alphas corresponding to the points on the boundary points (for which $\\|\\phi(x_i) - h\\|^2 = R$), outlier points and inlier points, respectively. Analogically, denote $C_i^B$, $C_i^O$, $C_i^I$. Then from $(1)$ have:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_i\\alpha_i^B + \\sum\\limits_i\\alpha_i^O + \\sum\\limits_i\\alpha_i^I = 1\n",
    "$$\n",
    "\n",
    "But from 7.1 1) we have that each $\\alpha_i^I = 0$, and from 7.2 1) we have that each $\\alpha_i^O = \\frac{C_i^O}{\\nu}$. Substituting this, we obtain:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_i\\alpha_i^B + \\sum\\limits_i\\frac{C_i^O}{\\nu} = 1 \\Rightarrow \\sum\\limits_iC_i^O = \\nu - \\nu\\sum\\limits_i\\alpha_i^B\n",
    "$$\n",
    "\n",
    "Notice that since all $\\alpha_i \\geq 0$ by dual constraints, we have that $\\sum\\limits_i\\alpha_i^B \\geq 0$, and hence:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_iC_i^O \\leq \\nu,\\qquad\\text{q.e.d.}\n",
    "$$\n",
    "\n",
    "Let's also derive upper bound for $C_i^I$ for the task 7.1 2). According to the initial problem, all weights sum to one:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_iC_i^B + \\sum\\limits_iC_i^O + \\sum\\limits_iC_i^I = 1\n",
    "$$\n",
    "\n",
    "Substituting $\\sum\\limits_iC_i^O = \\nu - \\nu\\sum\\limits_i\\alpha_i^B$ here, we obtain:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_iC_i^B + \\nu - \\nu\\sum\\limits_i\\alpha_i^B + \\sum\\limits_iC_i^I = 1 \\Rightarrow \\sum\\limits_iC_i^I = 1 - \\nu - \\left(\\sum\\limits_iC_i^B - \\nu\\sum\\limits_i\\alpha_i^B\\right)\n",
    "$$\n",
    "\n",
    "Finally, notice that for all $\\alpha_i$ it holds that $\\alpha_i \\leq \\frac{C_i}{\\nu}$. Thus, $\\nu\\sum\\limits_i\\alpha_i^B \\leq \\sum\\limits_iC_i^B$, and therefore we obtain\n",
    "\n",
    "$$\n",
    "\\sum\\limits_iC_i^I \\leq 1 - \\nu,\\qquad\\text{q.e.d.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.3: Very small $\\nu$ (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $\\nu < C_i$ for all $i=1,\\,\\ldots,\\,l$. Show that\n",
    "there are **no outliers** in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consider a set of outliers $A = \\{x_i \\colon \\|\\phi(x_i) - h \\|^2 > R\\}$. Suppose that $A \\neq \\emptyset$. Then, we have a derived in 7.2 2) upper bound on the sum of weights $C_i^O$ corresponding to the points $x_i$:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_iC_i^O \\leq \\nu\n",
    "$$\n",
    "\n",
    "Suppose that $|A| = 1$. Consider $x_j \\in A$. Then the sum of weights $C_i^O$ consists of only one summand:\n",
    "\n",
    "$$\n",
    "C_j^O \\leq \\nu\n",
    "$$\n",
    "\n",
    "By the task, $\\nu < C_i$ for all $i = 1, \\ldots, \\ell$. Taking $C_i = C_j^O$, we have $\\nu < C_j^O$, which is a contradiction to the shown upper bound.\n",
    "\n",
    "Now suppose that $|A| > 1$. Again consider $x_j \\in A$, take $C_i = C_j^O$ and get:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_iC_i^O \\leq \\nu < C_j^O \\Rightarrow \\sum\\limits_{i \\neq j}C_i^O < 0\n",
    "$$\n",
    "\n",
    "So, we obtained that a sum of weights corresponding to all $x_i \\in A\\backslash\\{x_j\\}$ is negative, which can't be true because all $C_i > 0$. It is a contradiction!\n",
    "\n",
    "Therefore, $A = \\emptyset$, q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.4 (1 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $C_i = \\tfrac1l$. Please, describe how $\\nu$ is related to the\n",
    "outliers in the datset, given the analysis above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Begin Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this case, $\\nu$ bounds a share of outliers among the whole sample:\n",
    "\n",
    "$$\n",
    "\\sum\\limits_iC_i^O = \\frac{|A|}{\\ell} \\leq \\nu\n",
    "$$\n",
    "\n",
    "So, the number of outliers $|A|$ is bounded in the following way:\n",
    "\n",
    "$$\n",
    "|A| \\leq \\nu\\ell,\n",
    "$$\n",
    "\n",
    "and if $\\nu < \\frac{1}{\\ell}$, then there are no outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
