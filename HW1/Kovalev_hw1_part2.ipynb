{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem just right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "* The are two problems with \\* mark - they are not obligatory. You can get **EXTRA POINTS** for solving them.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Ridge Regression (1 point)\n",
    "Let us consider the problem of linear ridge regression for data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{d\\times 1}$. Let the objects have positive **sample weights** $v_{i}>0$, i.e. the optimization problem is\n",
    "$$\\sum_{i=1}^{n}v_{i}\\cdot L(y_{i}, \\hat{y}_{i})+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}=\\sum_{i=1}^{n}v_{i}\\cdot (\\langle\\boldsymbol{w},\\boldsymbol{x}_{i}\\rangle-y_{i})^{2}+\\frac{\\lambda}{2}\\|w\\|_{2}^{2}\\rightarrow \\min_{\\boldsymbol{w}}.$$\n",
    "This problem reduces to classical linear ridge regression when $v_{i}\\equiv 1$. The matrix form of the problem is\n",
    "$$(Xw-y)^{\\top}V(Xw-y)+\\frac{\\lambda}{2}w^{\\top}w\\rightarrow\\min_{w},$$\n",
    "where $V=V^{\\top}\\in\\mathbb{R}^{n\\times n}$ is the diagonal matrix with diagonal elements $v_{1},\\dots, v_{n}$. Note that the quadratic problem is still convex (w.r.t. $\\boldsymbol{w}$), thus, the solution is unique. Solve this problem for any (symmetric) positive-definite matrix $V$ (not just diagonal) and provide the answer in the matrix form.\n",
    "### Your solution:\n",
    "\n",
    "$$\n",
    "L(w) = (Xw-y)^{\\top}V(Xw-y)+\\frac{\\lambda}{2}w^{\\top}w\\rightarrow\\min_{w}\n",
    "$$\n",
    "Let's calculate the derivative of $L(w)$ by $w$, compare it with zero and find optimal $w$:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial L}{\\partial w} = X^TV(Xw - y) + X^TV^T(Xw - y) + \\lambda w = 0\\Rightarrow\\\\\n",
    "&& \\Rightarrow [V \\in \\mathbb{S}_{++}] \\Rightarrow 2X^TV(Xw - y) + \\lambda w = 0\\Rightarrow\\\\\n",
    "&&\\Rightarrow \\left(2X^TVX + \\lambda I\\right)w = 2X^TVy \\Rightarrow\\\\\n",
    "&&\\Rightarrow w = 2\\left(2X^TVX + \\lambda I\\right)^{-1}X^TVy \\Rightarrow\\\\\n",
    "&&\\Rightarrow w = 2\\left(2\\left(X^TVX + \\frac{\\lambda}{2}I\\right)\\right)^{-1}X^TVy \\Rightarrow\\\\\n",
    "&&\\Rightarrow \\hat{w} = \\left(X^TVX + \\frac{\\lambda}{2} I\\right)^{-1}X^TVy\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "Let's check that $\\hat{w}$ is really a minimum:\n",
    "$$\n",
    "\\frac{\\partial^2 L}{\\partial w^2} = 2X^TVX + \\lambda I \\succeq 0,\n",
    "$$\n",
    "because $V$ is positive definite and $\\lambda \\geq 0$, q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Logistic Regression (1 point)\n",
    "Let us consider the case when in the problem of binary classification the training set is lineary separable. Show that in this case the optimization problem for logistic regression **without L2-regularization** does not have optimum.\n",
    "### Your solution:\n",
    "\n",
    "Linear separability of the training set means that there exists such vector $w$ so that all margins $(w^Tx_i)y_i$ will be positive. Then notice that all margins stay positive after multiplying vector $w$ by any positive constant $k$.\n",
    "\n",
    "Now consider optimization problem for logistic regression:\n",
    "\n",
    "$$\n",
    "L(w) = \\sum\\limits_{i=1}^m\\log\\left(1 + e^{-\\left(w^Tx_i\\right)y_i}\\right) \\rightarrow \\min_w\n",
    "$$\n",
    "\n",
    "If we are substituting $w$ by $kw$ and $k \\rightarrow \\infty$, the power of the exponent is approaching $-\\infty$, because all margins are positive, and a norm of weights vector $w$ can grow unlimitedly large. Therefore, for each $i = 1, \\ldots, m$ it is true that $\\lim\\limits_{\\|w\\| \\to \\infty}e^{-\\left(w^Tx_i\\right)y_i} = 0$, and hence $\\lim\\limits_{\\|w\\| \\to \\infty}L(w) = 0$ which is its lower border, because $L(w)$ is a sum of logarithms of number which are not lower than $1$. Thus, there is no optimum in such case, q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Bayesian Naive Classifier (1 point)\n",
    "Show that in case of $d$ binary-valued features $x_{j}\\in\\{0, 1\\}$ (for $j=1,2,\\dots,d$) Bayesian Naive Classifier's decision rule is linear.\n",
    "### Your solution:\n",
    "\n",
    "Suppose that we have $K$ classes $C_1, \\ldots, C_k$. A decision rule of Bayesian Naive Classifier looks like this:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname{argmax}_{k \\in \\{1, \\ldots, K\\}}p(C_k)\\prod\\limits_{j=1}^dp\\left(x_j\\mid C_k\\right)\n",
    "$$\n",
    "\n",
    "Since we are taking the argmax, it doesn't change anything if we take a logarithm of $f(x)$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\operatorname{argmax}_{k \\in \\{1, \\ldots, K\\}} \\log\\left(p(C_k)\\prod\\limits_{j=1}^dp\\left(x_j\\mid C_k\\right)\\right) = \\operatorname{argmax}_{k \\in \\{1, \\ldots, K\\}} \\log p(C_k) + \\sum\\limits_{j=1}^d\\log p\\left(x_j\\mid C_k\\right)\n",
    "$$\n",
    "\n",
    "Let's consider the function under the argmax:\n",
    "\n",
    "$$\n",
    "f(x) = \\log p(C_k) + \\sum\\limits_{j=1}^d\\log p\\left(x_j\\mid C_k\\right)\n",
    "$$\n",
    "\n",
    "Since $x_1, \\ldots, x_d$ are binary features, we can represent $f(x)$ as following:\n",
    "\n",
    "$$\n",
    "f(x) = \\log p(C_k) + \\sum\\limits_{j=1}^d\\left(I\\{x_j = 0\\}\\log p\\left(x_j = 0\\mid C_k\\right) + I\\{x_j = 1\\}\\log p\\left(x_j = 1\\mid C_k\\right)\\right)\n",
    "$$\n",
    "\n",
    "Now notice that $\\log p(C_k)$, $\\log p\\left(x_j = 0\\mid C_k\\right)$, $\\log p\\left(x_j = 1\\mid C_k\\right)$ are numbers which might be computed according to the training set. Denoting them as $a$, $b_j$ and $c_j$, respectively, we obtain:\n",
    "\n",
    "$$\n",
    "f(x) = a + \\sum\\limits_{j=1}^d\\left(b_jI\\{x_j = 0\\} + c_jI\\{x_j = 1\\}\\right) = a + \\sum\\limits_{j=1}^dc_j + \\sum\\limits_{j=1}^d\\left(b_j - c_j\\right)I\\{x_j = 0\\},\n",
    "$$\n",
    "\n",
    "which is a linear function of $x_1, \\ldots, x_d$, q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)\\bigg],$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:\n",
    "\n",
    "Let's consider a following part of the first formula: $\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]$. It represents a total number of incorrect answers on test data $X$ (which consists of $k$ objects, because in the limit of the first sum in the first formula there is $|X|=k$) when a classifier is fitted on $X^{n}\\setminus X$. Since we are considering the 1-nearest-neighbor classifier, an error on some object $x_i \\in X$ may happen only in case if the closest neighbor to it in the training set $X^{n}\\setminus X$ has a different label, which is not equal to $y_i$. Notice that it may be not its closest neighbor in $X^n$, because its closest neighbor could get in the test data. In general, if $m - 1$ nearest neighbors of $x_i \\in X^n$ got into $X$ and the $m$-th one got into $X^{n}\\setminus X$, then the result of the algorithm $f_{X^{n}\\setminus X}( x_{i})$ will depend only on $y_i^m$. The total number of possible variants to divide $X^n$ on train/test in this manner is $C_{n-1-m}^{k-m} = C_{n-1-m}^{n-k-1}$, because $m$ objects in $X$ and $1$ object in $X^{n}\\setminus X$ are fixed, and we are choosing the remaining $k - m$ objects for the test set $X$ from total $n - 1 - m$ unfixed objects. Notice that such process of counting is the same for each $m = 1, \\ldots, k$ and for each $x_i \\in X^n$.\n",
    "\n",
    "Therefore, changing the first formula according to the reasonings above and reordering the summations, we obtain:\n",
    "\n",
    "$$\n",
    "L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)\\bigg] = \\frac{1}{k C_{n}^{k}}\\sum\\limits_{i = 1}^n\\sum\\limits_{m=1}^k[y_{i}\\neq y_{i}^{m}]C_{n-1-m}^{n-k-1} = \\frac{1}{k C_{n}^{k}}\\sum\\limits_{m=1}^k\\sum\\limits_{i = 1}^n[y_{i}\\neq y_{i}^{m}]C_{n-1-m}^{n-k-1}\n",
    "$$\n",
    "\n",
    "The last thing to notice is that we can rewrite the denominator in the following way:\n",
    "\n",
    "$$\n",
    "kC_n^k = k\\frac{n!}{k!(n - k)!} = \\frac{n!}{(k - 1)!(n - k)!} = n\\frac{(n - 1)!}{(k - 1)!(n - k)!} = nC_{n - 1}^{k - 1}\n",
    "$$\n",
    "\n",
    "Hence, finally, we obtain the desired formula:\n",
    "\n",
    "$$\n",
    "L_{k}OCV = \\frac{1}{nC_{n - 1}^{k - 1}}\\sum\\limits_{m=1}^k\\sum\\limits_{i = 1}^n[y_{i}\\neq y_{i}^{m}]C_{n-1-m}^{n-k-1} = \\sum_{m=1}^{k}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}}, \\qquad\\text{q.e.d.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability of object $x_{i}$ to be present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:\n",
    "\n",
    "Let's compute the probability of object $x_i$ not to be present in $\\hat{X}^n$. It means that each time (of $n$ total) some other object (of $n-1$ total objects) has been taken to the subsample. The total number of such variants is $(n - 1)^n$. The total number of all possible variants to form a subsample is $n^n$. Therefore, the desired probability of object $x_i$ to be present in $\\hat{X}^n$ can be represented the following way:\n",
    "\n",
    "$$\n",
    "p = 1 - \\frac{(n - 1)^n}{n^n} = 1 - \\left(\\frac{n - 1}{n}\\right)^n = 1 - \\left(1 - \\frac{1}{n}\\right)^n\n",
    "$$\n",
    "\n",
    "Let's find its limit for $n$ approaching the infinity:\n",
    "\n",
    "$$\n",
    "\\lim\\limits_{n \\to \\infty}p = \\lim\\limits_{n \\to \\infty}\\left(1 - \\left(1 - \\frac{1}{n}\\right)^n\\right) = 1 - \\lim\\limits_{n \\to \\infty}\\left(1 + \\frac{1}{n}\\right)^{-n} = 1 - \\left(\\lim\\limits_{n \\to \\infty}\\left(1 + \\frac{1}{n}\\right)^n\\right)^{-1} = 1 - e^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1+1=3 points)\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample, i.e. $\\frac{1}{n}\\sum_{i=1}^{n}L(y_{i}, \\hat{y})\\rightarrow\\min$. We consider three cases:\n",
    "1. Regression tree ($y_{i}\\in\\mathbb{R}$), absolute loss function $L(y,\\hat{y})=|y-\\hat{y}|$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}|y_{i}-\\hat{y}|$ is the median of labels: \n",
    "$$\\hat{y}=\\text{median}(y_{1},\\dots,y_{n}).$$\n",
    "In this case, for simplicity you may assume that $n$ is even (or odd, as you wish).\n",
    "2. Regression tree ($y_{i}\\in\\mathbb{R}$), squared loss function $L(y,\\hat{y})=(y-\\hat{y})^{2}$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y})^{2}$ is the mean of labels:\n",
    "$$\\hat{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i.$$\n",
    "3. Classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$), zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$. The optimal prediction that minimizes $\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq\\hat{y}]$ is the most frequent label:\n",
    "$$\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k].$$\n",
    "In this case, for simplicity you may assume that there is only one most frequent label.\n",
    "\n",
    "Suppose that that instead of using optimal prediction for this leaf we output the label from $y_{1},\\dots,y_{n}$ at random. What will happen with the (expected) loss on the training sample (will it increase/decrease/not change)? Prove your answer (separately for every case).\n",
    "### Your solution:\n",
    "\n",
    "1) Let's denote optimal loss as $\\hat{L} = \\frac{1}{n}\\sum\\limits_{i=1}^n|y_i - \\hat{y}|$, where $\\hat{y} = \\text{median}(y_{1},\\dots,y_{n})$. Let's calculate the expected loss from a random choice of labels $y_1, \\ldots, y_n$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}L = \\frac{1}{n}\\left(\\frac{1}{n}\\sum |y_i - y_1| + \\ldots + \\frac{1}{n}\\sum |y_i - y_n|\\right) = \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n|y_i - y_j|\n",
    "$$\n",
    "\n",
    "Notice that since $\\hat{L}$ is the optimal loss, it will be not greater than any loss $\\frac{1}{n}\\sum\\limits_{i=1}^n|y_i - y_j|$. So:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}L = \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n|y_i - y_j| \\geq \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n|y_i - \\hat{y}| = \\frac{1}{n}\\sum\\limits_{j=1}^n\\hat{L} = \\hat{L}\\qquad\\qquad (1)\n",
    "$$\n",
    "\n",
    "Moreover, $\\hat{L} = \\frac{1}{n}\\sum\\limits_{i=1}^n|y_i - y_j|$ for some $j$ only if $\\hat{y} = y_j$ because it is optimal. Therefore, the inequality in (1) becomes an equality only in case if all labels are the same: $y_1 = \\ldots = y_n$. So, the loss will not change only if all labels are equal, otherwise it will increase.\n",
    "\n",
    "2) Let's denote optimal loss as $\\hat{L} = \\frac{1}{n}\\sum\\limits_{i=1}^n\\left(y_i - \\hat{y}\\right)^2$, where $\\hat{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_i$. Let's calculate the expected loss from a random choice of labels $y_1, \\ldots, y_n$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}L = \\frac{1}{n}\\left(\\frac{1}{n}\\sum \\left(y_i - y_1\\right)^2 + \\ldots + \\frac{1}{n}\\sum \\left(y_i - y_n\\right)^2\\right) = \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n\\left(y_i - y_j\\right)^2\n",
    "$$\n",
    "\n",
    "Notice that since $\\hat{L}$ is the optimal loss, it will be not greater than any loss $\\frac{1}{n}\\sum\\limits_{i=1}^n\\left(y_i - y_j\\right)^2$. So:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}L = \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n\\left(y_i - y_j\\right)^2 \\geq \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n\\left(y_i - \\hat{y}\\right)^2 = \\frac{1}{n}\\sum\\limits_{j=1}^n\\hat{L} = \\hat{L}\\qquad\\qquad (2)\n",
    "$$\n",
    "\n",
    "Moreover, $\\hat{L} = \\frac{1}{n}\\sum\\limits_{i=1}^n\\left(y_i - y_j\\right)^2$ for some $j$ only if $\\hat{y} = y_j$ because it is optimal. Therefore, the inequality in (2) becomes an equality only in case if all labels are the same: $y_1 = \\ldots = y_n$. So, the loss will not change only if all labels are equal, otherwise it will increase.\n",
    "\n",
    "3) Let's denote optimal loss as $\\hat{L} = \\frac{1}{n}\\sum\\limits_{i=1}^n[y_{i}\\neq\\hat{y}]$, where $\\hat{y}=\\underset{k=1,2,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k]$. Let's calculate the expected loss from a random choice of labels $y_1, \\ldots, y_n$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}L = \\frac{1}{n}\\left(\\frac{1}{n}\\sum [y_{i}\\neq y_1] + \\ldots + \\frac{1}{n}\\sum [y_{i}\\neq y_n]\\right) = \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n[y_{i}\\neq y_j]\n",
    "$$\n",
    "\n",
    "Notice that since $\\hat{L}$ is the optimal loss, it will be not greater than any loss $\\frac{1}{n}\\sum\\limits_{i=1}^n[y_{i}\\neq y_j]$. So:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}L = \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n[y_{i}\\neq y_j] \\geq \\frac{1}{n}\\sum\\limits_{j=1}^n\\frac{1}{n}\\sum\\limits_{i=1}^n[y_{i}\\neq \\hat{y}] = \\frac{1}{n}\\sum\\limits_{j=1}^n\\hat{L} = \\hat{L}\\qquad\\qquad (3)\n",
    "$$\n",
    "\n",
    "Moreover, $\\hat{L} = \\frac{1}{n}\\sum\\limits_{i=1}^n[y_{i}\\neq y_j]$ for some $j$ only if $\\hat{y} = y_j$ because it is optimal. Therefore, the inequality in (3) becomes an equality only in case if all labels are the same: $y_1 = \\ldots = y_n$. So, the loss will not change only if all labels are equal, otherwise it will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7*. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:\n",
    "\n",
    "Let's consider a threshold between $i$-$1$-th and $i$-th objects. Let's calculate the corresponding coordinates in FPR/TPR:\n",
    "\n",
    "$$\n",
    "FPR_i = \\frac{FP_i}{FP + TN} = \\frac{\\sum\\limits_{j = i}^n[y_j = 0]}{n - n_1}\n",
    "$$\n",
    "$$\n",
    "TPR_i = \\frac{TP_i}{TP + FN} = \\frac{\\sum\\limits_{j = i}^n[y_j = 1]}{n_1}\n",
    "$$\n",
    "\n",
    "The area under ROC-curve can be computed as a sum of areas of rectangles. Let's consider a step $i \\to i+1$. One of two things may happen: either $TPR$ increases and $FPR$ stays the same or, conversely, $FPR$ increases and $TPR$ stays the same. In the first case, let's suppose that the area of a corresponding rectangle is zero, and in the second it is equal to $TPR_{i + 1} \\left(FPR_{i + 1} - FPR_i\\right)$. Notice that this is zero is $FPR$ didn't change. Hence we can represent AUC-ROC as follows:\n",
    "\n",
    "$$\n",
    "S_{\\text{ROC}}(f,X^{n}) = \\sum\\limits_{i=1}^nTPR_{i + 1}\\left(FPR_{i + 1} - FPR_i\\right) = \\sum\\limits_{i=1}^n\\frac{\\sum\\limits_{j = i + 1}^n[y_j = 1]}{n_1}\\left(\\frac{\\sum\\limits_{j = i + 1}^n[y_j = 0]}{n - n_1} - \\frac{\\sum\\limits_{j = i}^n[y_j = 0]}{n - n_1}\\right) = \\frac{1}{n_1\\left(n - n_1\\right)}\\sum\\limits_{i=1}^n[y_i = 0]\\sum\\limits_{j = i + 1}^n[y_j = 1]\n",
    "$$\n",
    "\n",
    "So, we may rewrite this as:\n",
    "\n",
    "$$\n",
    "S_{\\text{ROC}}(f,X^{n}) = \\frac{1}{n_1\\left(n - n_1\\right)}\\sum\\limits_{i < j}[y_i = 0][y_j = 1]\n",
    "$$\n",
    "\n",
    "Finally, notice that $[y_i = 0][y_j = 1] = 1$ iff $y_i = 0$, $y_j = 1$, and the same is true for $[y_i < y_j]$. Therefore we may substitute $[y_i < y_j]$ instead of $[y_i = 0][y_j = 1]$ and obtain a desired formula:\n",
    "\n",
    "$$\n",
    "S_{\\text{ROC}}(f,X^{n}) = \\frac{1}{n_1\\left(n - n_1\\right)}\\sum\\limits_{i < j}[y_i < y_j], \\qquad \\text{q.e.d.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernel Regression (1 point)\n",
    "Recall that the prediction of Kernel Ridge Regression fitted on data $X^{n}$ with the kernel $K(\\cdot, \\cdot)$ has the form $\\mathcal{K}(x)=\\sum_{i=1}^{n}\\alpha_{i}K(x, x_{i})$, where $\\alpha=(K+\\lambda I)^{-1}Y$ ($K_{ij}=K(x_{i},x_{j})$). The time complexity of computation of a prediction $\\mathcal{K}(x)$ for any point $x$ is $O(n)$, i.e. grows linearly with the size of the training set.\n",
    "\n",
    "Consider the bilinear kernel $K(x, x')=\\langle x, x'\\rangle$. For this kernel, the Kernel Regression is known to turn into simple linear ridge regression. However, for linear regression the computation time of prediction $\\mathcal{R}(x)=\\sum_{j=1}^{d}\\beta_{j}x^{j}$ is $O(d)$, where $d$ is the dimension of the feature space and does not grow with the training, which is a little bit controversary to the previous paragraph.\n",
    "\n",
    "In this task in order to show that the kernel regression with the bilinear kernel is indeed the linear ridge regression, you have to prove that the predictions exactly match by **direct comparison**.\n",
    "### Your solution:\n",
    "\n",
    "Linear ridge regression:\n",
    "\n",
    "$$\n",
    "\\hat{f}(x) = \\sum\\limits_{i=1}^n\\alpha_i\\langle x, x_i\\rangle,\n",
    "$$\n",
    "\n",
    "where $\\alpha = \\left(XX^T + \\lambda I\\right)^{-1}$.\n",
    "\n",
    "Kernel ridge regression with bilinear kernel:\n",
    "\n",
    "$$\n",
    "\\mathcal{K}(x)=\\sum_{i=1}^{n}\\alpha_{i}\\langle x, x_i\\rangle,\n",
    "$$\n",
    "\n",
    "where $\\alpha=\\left(K+\\lambda I\\right)^{-1}Y$.\n",
    "\n",
    "As can be seen from the formulas above, the only thing we have to show in order to prove that $\\hat{f}(x) = \\mathcal{K}(x)$ is that $XX^T = K$. Let's compute $XX^T$:\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}x_1\\\\\\vdots\\\\x_n\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "X^T = \\begin{pmatrix}x_1 & \\cdots & x_n\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "XX^T = \\begin{pmatrix}\\langle x_1, x_1\\rangle & \\langle x_1, x_2\\rangle & \\cdots & \\langle x_1, x_n\\rangle\\\\ \\langle x_2, x_1\\rangle & \\langle x_2, x_2\\rangle & \\cdots & \\langle x_2, x_n\\rangle\\\\ \\vdots& \\vdots & \\ddots & \\vdots\\\\ \\langle x_n, x_1\\rangle & \\langle x_n, x_2\\rangle & \\cdots & \\langle x_n, x_n\\rangle\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "But this is exactly matrix $K$, because by definition $K = [K(x_i, x_j)]_{ij} = [\\langle x_i, x_j\\rangle]_{ij}$. So, $XX^T = K$, and thus $\\hat{f}(x) = \\mathcal{K}(x)$, q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that the function $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite kernel.\n",
    "### Your solution:\n",
    "\n",
    "Notice that:\n",
    "\n",
    "$$\n",
    "K(x, x') = e^{-\\|x - x'\\|^2} = e^{-\\langle x - x', x - x'\\rangle} = e^{-\\|x\\|^2 + 2\\langle x, x'\\rangle - \\|x'\\|^2} = e^{-\\|x\\|^2}e^{2\\langle x, x'\\rangle}e^{- \\|x'\\|^2}\n",
    "$$\n",
    "\n",
    "Let's show that $K_{\\ell}(x, x') = 2\\langle x, x'\\rangle$ is a positive definite kernel. To do this, consider some vector $z$ and matrix $K_{\\ell} = [K_{\\ell}(x_i, x_j)]_{ij}$. Let's show that this matrix is positive semi-definite:\n",
    "\n",
    "$$\n",
    "z^*K_{\\ell}z = \\sum\\limits_{i=1}^nz_i\\sum\\limits_{j=1}^n\\bar{z}_jK_{\\ell}(x_i, x_j) = 2\\sum\\limits_{i = 1}^n\\sum\\limits_{j=1}^nz_i\\bar{z}_j\\langle x_i, x_j\\rangle = 2\\langle\\sum\\limits_{i=1}^nz_ix_i, \\sum\\limits_{j=1}^nz_jx_j\\rangle = 2\\left\\|\\sum\\limits_{i=1}^nz_ix_i\\right\\|^2 \\geq 0\n",
    "$$\n",
    "\n",
    "So, $K_{\\ell}$ is a positive semi-definite matrix, and hence $K_{\\ell}(x, x') = 2\\langle x, x'\\rangle$ is a positive definite kernel. Now let's consider $K_e(x, x') = e^{K_{\\ell}(x, x')}$. Using Maclaurin series of the exponential function, we obtain that $K_e(x, x') = \\sum\\limits_{i=0}^{\\infty}\\frac{K_{\\ell}^i (x, x')}{i!} = \\lim\\limits_{m \\to \\infty}\\sum\\limits_{i=0}^{m}\\frac{K_{\\ell}^i (x, x')}{i!}$. By general properties of positive definite kernels, $\\sum\\limits_{i=0}^{m}\\frac{K_{\\ell}^i (x, x')}{i!}$ is a positive definite kernel since each $i! \\geq 0$, and also a limit of this sum is a positive definite kernel because this Maclaurin series converges. Thus, it was shown that $K_e(x, x') = e^{K_{\\ell}(x, x')}$ is a positive definite kernel.\n",
    "\n",
    "Then, since $K_e(x, x') = e^{K_{\\ell}(x, x')}$ is a positive definite kernel, there exists some feature map $\\Phi \\colon X \\rightarrow F$ such that $K_e(x, x') = \\langle \\Phi(x), \\Phi(x')\\rangle_F$, where $F$ is a Hilbert space with inner product $\\langle\\cdot, \\cdot\\rangle_F$. Therefore, $K(x, x')$ may be represented as follows:\n",
    "\n",
    "$$\n",
    "K(x, x') = e^{-\\|x\\|^2}e^{2\\langle x, x'\\rangle}e^{- \\|x'\\|^2} = e^{-\\|x\\|^2}\\langle \\Phi(x), \\Phi(x')\\rangle_F e^{- \\|x'\\|^2} = \\langle e^{-\\|x\\|^2}\\Phi(x), e^{- \\|x'\\|^2}\\Phi(x')\\rangle_F\n",
    "$$\n",
    "\n",
    "Now let's prove that it is a positive definite kernel. Consider some vector $z$ and matrix $K = [K(x_i, x_j)]_{ij}$. Let's show that this matrix is positive semi-definite:\n",
    "\n",
    "$$\n",
    "z^*Kz = \\sum\\limits_{i=1}^nz_i\\sum\\limits_{j=1}^n\\bar{z}_jK(x_i, x_j) = \\sum\\limits_{i = 1}^n\\sum\\limits_{j=1}^nz_i\\bar{z}_j\\langle e^{-\\|x_i\\|^2}\\Phi(x_i), e^{- \\|x_j\\|^2}\\Phi(x_j)\\rangle_F = \\langle\\sum\\limits_{i = 1}^nz_ie^{-\\|x_i\\|^2}\\Phi(x_i), \\sum\\limits_{ji = 1}^nz_je^{-\\|x_j\\|^2}\\Phi(x_j)\\rangle_F = \\left\\|\\sum\\limits_{i = 1}^nz_ie^{-\\|x_i\\|^2}\\Phi(x_i)\\right\\|^2 \\geq 0\n",
    "$$\n",
    "\n",
    "Therefore, $K$ is a positive semi-definite matrix, and so $K(x, x') = e^{-\\|x - x'\\|^2}$ is a positive definite kernel, q.e.d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10*. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:\n",
    "\n",
    "Notice that a separating hyperplane depends only on support vectors, so if we exclude some observation $(x_{i},y_{i})$ from data, this hyperplane won't change unless $(x_{i},y_{i})$ was a support vector. Also notice that missclassifications can only occur if the corresponding objects are support vectors, therefore their total number can't be higher than number of support vectors:\n",
    "\n",
    "$$\n",
    "[y_{i}\\neq f_{i}(x_{i})] \\leq |SV| \\Rightarrow \\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq f_{i}(x_{i})]\\leq \\frac{|SV|}{n}, \\qquad \\text{q.e.d.}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
